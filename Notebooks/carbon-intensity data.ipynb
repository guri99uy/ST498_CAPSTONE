{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ba8b653",
   "metadata": {},
   "source": [
    "# Carbon Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c32faa",
   "metadata": {},
   "source": [
    "### No data for these dates \n",
    "\n",
    "October 20, 2023: Missing: 22:00, 22:30, 23:00, 23:30 (4 slots)\n",
    "\n",
    "October 21, 2023: Missing: ENTIRE DAY (all 48 half-hour slots from 00:00 to 23:30)\n",
    "\n",
    "June 11, 2024: Missing: 23:00, 23:30 (2 slots)\n",
    "\n",
    "June 12, 2024: Missing: 00:00 through 14:00 (29 slots)\n",
    "Available: 14:30 through 23:30 (19 slots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c0351037",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "834bcb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_range_chunked(start_date, end_date):\n",
    "    max_days = 30  # chunk size\n",
    "    start = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    while start <= end:\n",
    "        chunk_end = min(start + timedelta(days=max_days - 1), end)\n",
    "        s = start.strftime(\"%Y-%m-%dT00:00Z\")\n",
    "        e = chunk_end.strftime(\"%Y-%m-%dT23:30Z\")\n",
    "        \n",
    "        print(f\"Fetching data from {s} to {e}...\")\n",
    "        url = f\"https://api.carbonintensity.org.uk/intensity/{s}/{e}\"\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        records = []\n",
    "        for item in data['data']:\n",
    "            records.append({\n",
    "                'from': pd.to_datetime(item['from']),\n",
    "                'to': pd.to_datetime(item['to']),\n",
    "                'forecast': item['intensity']['forecast'],\n",
    "                'actual': item['intensity']['actual'],\n",
    "                'index': item['intensity']['index']\n",
    "            })\n",
    "        \n",
    "        df_chunk = pd.DataFrame(records)\n",
    "        all_data.append(df_chunk)\n",
    "        \n",
    "        start = chunk_end + timedelta(days=1)\n",
    "    \n",
    "    df_all = pd.concat(all_data).reset_index(drop=True)\n",
    "    df_all['day_of_week'] = df_all['to'].dt.day_name()\n",
    "    return df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9b79fdd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching continuous carbon data for entire study period...\n",
      "Fetching data from 2023-01-01T00:00Z to 2023-01-30T23:30Z...\n",
      "Fetching data from 2023-01-31T00:00Z to 2023-03-01T23:30Z...\n",
      "Fetching data from 2023-03-02T00:00Z to 2023-03-31T23:30Z...\n",
      "Fetching data from 2023-04-01T00:00Z to 2023-04-30T23:30Z...\n",
      "Fetching data from 2023-05-01T00:00Z to 2023-05-30T23:30Z...\n",
      "Fetching data from 2023-05-31T00:00Z to 2023-06-29T23:30Z...\n",
      "Fetching data from 2023-06-30T00:00Z to 2023-07-29T23:30Z...\n",
      "Fetching data from 2023-07-30T00:00Z to 2023-08-28T23:30Z...\n",
      "Fetching data from 2023-08-29T00:00Z to 2023-09-27T23:30Z...\n",
      "Fetching data from 2023-09-28T00:00Z to 2023-10-27T23:30Z...\n",
      "Fetching data from 2023-10-28T00:00Z to 2023-11-26T23:30Z...\n",
      "Fetching data from 2023-11-27T00:00Z to 2023-12-26T23:30Z...\n",
      "Fetching data from 2023-12-27T00:00Z to 2024-01-25T23:30Z...\n",
      "Fetching data from 2024-01-26T00:00Z to 2024-02-24T23:30Z...\n",
      "Fetching data from 2024-02-25T00:00Z to 2024-03-25T23:30Z...\n",
      "Fetching data from 2024-03-26T00:00Z to 2024-04-24T23:30Z...\n",
      "Fetching data from 2024-04-25T00:00Z to 2024-05-24T23:30Z...\n",
      "Fetching data from 2024-05-25T00:00Z to 2024-06-23T23:30Z...\n",
      "Fetching data from 2024-06-24T00:00Z to 2024-07-23T23:30Z...\n",
      "Fetching data from 2024-07-24T00:00Z to 2024-08-22T23:30Z...\n",
      "Fetching data from 2024-08-23T00:00Z to 2024-09-21T23:30Z...\n",
      "Fetching data from 2024-09-22T00:00Z to 2024-10-21T23:30Z...\n",
      "Fetching data from 2024-10-22T00:00Z to 2024-11-20T23:30Z...\n",
      "Fetching data from 2024-11-21T00:00Z to 2024-12-20T23:30Z...\n",
      "Fetching data from 2024-12-21T00:00Z to 2025-01-01T23:30Z...\n",
      "Total carbon records fetched: 35014\n",
      "Date range: 2022-12-31 23:30:00+00:00 to 2025-01-01 23:30:00+00:00\n",
      "Missing actual values: 5\n"
     ]
    }
   ],
   "source": [
    "# FETCH CONTINUOUS DATA FOR ENTIRE STUDY PERIOD\n",
    "print(\"Fetching continuous carbon data for entire study period...\")\n",
    "carbon_continuous = fetch_range_chunked(\"2023-01-01\", \"2025-01-01\")\n",
    "\n",
    "print(f\"Total carbon records fetched: {len(carbon_continuous)}\")\n",
    "print(f\"Date range: {carbon_continuous['from'].min()} to {carbon_continuous['to'].max()}\")\n",
    "print(f\"Missing actual values: {carbon_continuous['actual'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ee56b70b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 5 records with missing carbon intensity...\n",
      "Clean carbon records: 35009\n"
     ]
    }
   ],
   "source": [
    "# Clean missing carbon data after fetching\n",
    "if carbon_continuous['actual'].isna().sum() > 0:\n",
    "    print(f\"Removing {carbon_continuous['actual'].isna().sum()} records with missing carbon intensity...\")\n",
    "    carbon_continuous = carbon_continuous.dropna(subset=['actual']).copy()\n",
    "    print(f\"Clean carbon records: {len(carbon_continuous)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "42148997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved continuous carbon data to 'carbon_continuous.csv\n",
      "\n",
      "Sample of continuous carbon data:\n",
      "                       from                        to  forecast  actual index  \\\n",
      "0 2022-12-31 23:30:00+00:00 2023-01-01 00:00:00+00:00        75    65.0   low   \n",
      "1 2023-01-01 00:00:00+00:00 2023-01-01 00:30:00+00:00        73    72.0   low   \n",
      "2 2023-01-01 00:30:00+00:00 2023-01-01 01:00:00+00:00        63    80.0   low   \n",
      "3 2023-01-01 01:00:00+00:00 2023-01-01 01:30:00+00:00        71    72.0   low   \n",
      "4 2023-01-01 01:30:00+00:00 2023-01-01 02:00:00+00:00        76    65.0   low   \n",
      "\n",
      "  day_of_week  \n",
      "0      Sunday  \n",
      "1      Sunday  \n",
      "2      Sunday  \n",
      "3      Sunday  \n",
      "4      Sunday  \n"
     ]
    }
   ],
   "source": [
    "# Save the continuous dataset\n",
    "carbon_continuous.to_csv(\"carbon_continuous.csv\", index=False)\n",
    "print(\"Saved continuous carbon data to 'carbon_continuous.csv\")\n",
    "\n",
    "# Display sample data\n",
    "print(\"\\nSample of continuous carbon data:\")\n",
    "print(carbon_continuous.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823d7f47",
   "metadata": {},
   "source": [
    "# Merging the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8cfdd7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0.1: Define date ranges\n",
    "pre_start = pd.Timestamp(\"2023-02-01 00:00:00\")\n",
    "pre_end   = pd.Timestamp(\"2024-01-31 23:59:59\")\n",
    "\n",
    "post_start = pd.Timestamp(\"2024-04-01 00:00:00\")\n",
    "post_end   = pd.Timestamp(\"2024-12-31 23:59:59\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a76da4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_carbon_data_for_all_hours(carbon_df):\n",
    "    \"\"\"Process carbon data for all hours, filtered for electricity data periods only\"\"\"\n",
    "    carbon_df = carbon_df.copy()\n",
    "    carbon_df['from'] = pd.to_datetime(carbon_df['from']).dt.tz_localize(None)\n",
    "    \n",
    "    # Filter for periods where electricity data exists\n",
    "    pre_mask = (carbon_df['from'] >= pd.Timestamp(\"2023-02-02 00:00:00\")) & \\\n",
    "               (carbon_df['from'] <= pd.Timestamp(\"2024-01-31 23:59:59\"))\n",
    "    post_mask = (carbon_df['from'] >= pd.Timestamp(\"2024-04-01 00:00:00\")) & \\\n",
    "                (carbon_df['from'] <= pd.Timestamp(\"2024-12-31 23:59:59\"))\n",
    "    \n",
    "    carbon_df = carbon_df[pre_mask | post_mask].copy()\n",
    "    carbon_df['datetime_30min'] = carbon_df['from']\n",
    "    \n",
    "    carbon_30min = carbon_df[['datetime_30min', 'actual']].copy()\n",
    "    carbon_30min.rename(columns={'actual': 'actual_intensity'}, inplace=True)\n",
    "    \n",
    "    return carbon_30min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7f934794",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_electricity_with_carbon_full_day(file_name, start_date, end_date, treatment, period, carbon_data):\n",
    "    \"\"\"Process electricity data and merge with carbon data for all hours\"\"\"\n",
    "    df = pd.read_csv(file_name)\n",
    "    if 'Unnamed: 0' in df.columns:\n",
    "        df = df.rename(columns={'Unnamed: 0': 'Time'})\n",
    "    else:\n",
    "        df.rename(columns={df.columns[0]: 'Time'}, inplace=True)\n",
    "    \n",
    "    df['Time'] = pd.to_datetime(df['Time'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    print(f\"Processing {file_name} -> Min Time: {df['Time'].min()}, Max Time: {df['Time'].max()}\")\n",
    "    \n",
    "    # Filter by time range\n",
    "    df = df[(df['Time'] >= start_date) & (df['Time'] <= end_date)]\n",
    "    \n",
    "    # Convert from wide to long format\n",
    "    df_long = df.melt(id_vars=['Time'], var_name='ANON_ID', value_name='ELEC_KWH')\n",
    "    df_long.drop_duplicates(subset=['Time', 'ANON_ID'], inplace=True)\n",
    "    \n",
    "    print(f\"  Electricity records: {len(df_long):,}\")\n",
    "    print(f\"  Missing electricity values: {df_long['ELEC_KWH'].isna().sum():,} ({df_long['ELEC_KWH'].isna().mean()*100:.2f}%)\")\n",
    "    \n",
    "    df_long['datetime_30min'] = df_long['Time']\n",
    "    \n",
    "    # Use inner join to keep only records where BOTH electricity and carbon data exist\n",
    "    df_with_carbon = pd.merge(df_long, carbon_data, on='datetime_30min', how='inner')\n",
    "    \n",
    "    print(f\"  Records after inner join: {len(df_with_carbon):,}\")\n",
    "    print(f\"  Records removed due to missing carbon data: {len(df_long) - len(df_with_carbon):,}\")\n",
    "    \n",
    "    df_with_carbon['carbon_emissions'] = df_with_carbon['ELEC_KWH'] * df_with_carbon['actual_intensity']\n",
    "    df_with_carbon['treatment'] = treatment\n",
    "    df_with_carbon['period'] = period\n",
    "    df_with_carbon['DateTime'] = df_with_carbon['Time']\n",
    "    \n",
    "    final_columns = ['ANON_ID', 'DateTime', 'ELEC_KWH', 'actual_intensity', 'carbon_emissions', 'treatment', 'period']\n",
    "    df_final = df_with_carbon[final_columns].copy()\n",
    "    \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "61d1d0a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carbon data processed: 30,550 records\n"
     ]
    }
   ],
   "source": [
    "# Process Carbon Data\n",
    "carbon_continuous = pd.read_csv('carbon_continuous.csv')\n",
    "all_carbon = process_carbon_data_for_all_hours(carbon_continuous)\n",
    "print(f\"Carbon data processed: {len(all_carbon):,} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8343a8d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing D:\\LSE\\Capstone Project\\FLASH data\\controlePreConsolide.csv -> Min Time: 2023-02-02 00:00:00, Max Time: 2024-01-31 23:30:00\n",
      "  Electricity records: 5,540,480\n",
      "  Missing electricity values: 51,770 (0.93%)\n",
      "  Records after inner join: 5,511,360\n",
      "  Records removed due to missing carbon data: 29,120\n",
      "Processing D:\\LSE\\Capstone Project\\FLASH data\\controleConsolide.csv -> Min Time: 2024-04-01 00:00:00, Max Time: 2024-12-31 23:30:00\n",
      "  Electricity records: 5,706,668\n",
      "  Missing electricity values: 481,188 (8.43%)\n",
      "  Records after inner join: 5,690,982\n",
      "  Records removed due to missing carbon data: 15,686\n",
      "Processing D:\\LSE\\Capstone Project\\FLASH data\\interventionPreConsolide.csv -> Min Time: 2023-02-02 00:00:00, Max Time: 2024-01-31 23:30:00\n",
      "  Electricity records: 6,722,577\n",
      "  Missing electricity values: 72,256 (1.07%)\n",
      "  Records after inner join: 6,687,360\n",
      "  Records removed due to missing carbon data: 35,217\n",
      "Processing D:\\LSE\\Capstone Project\\FLASH data\\interventionConsolide.csv -> Min Time: 2024-04-01 00:00:00, Max Time: 2024-12-31 23:30:00\n",
      "  Electricity records: 7,497,828\n",
      "  Missing electricity values: 517,321 (6.90%)\n",
      "  Records after inner join: 7,477,182\n",
      "  Records removed due to missing carbon data: 20,646\n",
      "\n",
      "Dataset shapes:\n",
      "df_control_pre: (5511360, 7)\n",
      "df_control_post: (5690982, 7)\n",
      "df_intervention_pre: (6687360, 7)\n",
      "df_intervention_post: (7477182, 7)\n"
     ]
    }
   ],
   "source": [
    "# Process Electricity Data\n",
    "electricity_folder = \"D:\\LSE\\Capstone Project\\FLASH data\" \n",
    "\n",
    "file_configs = [\n",
    "    (\"controlePreConsolide.csv\", pre_start, pre_end, \"Control\", \"Pre\"),\n",
    "    (\"controleConsolide.csv\", post_start, post_end, \"Control\", \"Post\"),\n",
    "    (\"interventionPreConsolide.csv\", pre_start, pre_end, \"Intervention\", \"Pre\"),\n",
    "    (\"interventionConsolide.csv\", post_start, post_end, \"Intervention\", \"Post\")\n",
    "]\n",
    "\n",
    "processed_datasets = {}\n",
    "dataset_names = [\"df_control_pre\", \"df_control_post\", \n",
    "                \"df_intervention_pre\", \"df_intervention_post\"]\n",
    "\n",
    "for i, (filename, start_date, end_date, treatment, period) in enumerate(file_configs):\n",
    "    file_path = os.path.join(electricity_folder, filename)\n",
    "    dataset_name = dataset_names[i]\n",
    "    \n",
    "    processed_datasets[dataset_name] = preprocess_electricity_with_carbon_full_day(\n",
    "        file_path, start_date, end_date, treatment, period, all_carbon)\n",
    "\n",
    "# Extract datasets\n",
    "df_control_pre = processed_datasets[\"df_control_pre\"]\n",
    "df_control_post = processed_datasets[\"df_control_post\"]\n",
    "df_intervention_pre = processed_datasets[\"df_intervention_pre\"]\n",
    "df_intervention_post = processed_datasets[\"df_intervention_post\"]\n",
    "\n",
    "print(\"\\nDataset shapes:\")\n",
    "for name, df in processed_datasets.items():\n",
    "    print(f\"{name}: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ec1965dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final dataset shape: (25366884, 7)\n",
      "Unique households: 1173\n",
      "\n",
      "Removed 1,120,464 records with missing electricity data\n",
      "Final clean dataset: 24,246,420 records\n"
     ]
    }
   ],
   "source": [
    "# Combine all datasets\n",
    "all_datasets = [df for df in [df_control_pre, df_control_post, df_intervention_pre, df_intervention_post] if not df.empty]\n",
    "final_df = pd.concat(all_datasets, ignore_index=True)\n",
    "\n",
    "print(f\"\\nFinal dataset shape: {final_df.shape}\")\n",
    "print(f\"Unique households: {final_df['ANON_ID'].nunique()}\")\n",
    "\n",
    "# Clean any remaining missing electricity data\n",
    "initial_records = len(final_df)\n",
    "final_df_clean = final_df.dropna(subset=['ELEC_KWH']).copy()\n",
    "records_removed = initial_records - len(final_df_clean)\n",
    "\n",
    "if records_removed > 0:\n",
    "    print(f\"\\nRemoved {records_removed:,} records with missing electricity data\")\n",
    "    print(f\"Final clean dataset: {len(final_df_clean):,} records\")\n",
    "else:\n",
    "    print(f\"\\nNo missing data found - dataset is clean: {len(final_df_clean):,} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "78cfe0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shorten ANON_IDs\n",
    "unique_ids = final_df_clean['ANON_ID'].unique()\n",
    "id_mapping = {long_id: f\"H{i}\" for i, long_id in enumerate(unique_ids)}\n",
    "final_df_clean['ANON_ID'] = final_df_clean['ANON_ID'].map(id_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "88ed79a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of final data:\n",
      "  ANON_ID            DateTime  ELEC_KWH  actual_intensity  carbon_emissions  \\\n",
      "0      H0 2023-02-02 00:00:00     0.140              64.0             8.960   \n",
      "1      H0 2023-02-02 00:30:00     0.138              64.0             8.832   \n",
      "2      H0 2023-02-02 01:00:00     0.128              66.0             8.448   \n",
      "3      H0 2023-02-02 01:30:00     0.149              65.0             9.685   \n",
      "4      H0 2023-02-02 02:00:00     0.137              66.0             9.042   \n",
      "\n",
      "  treatment period  \n",
      "0   Control    Pre  \n",
      "1   Control    Pre  \n",
      "2   Control    Pre  \n",
      "3   Control    Pre  \n",
      "4   Control    Pre  \n",
      "\n",
      "Saved to 'carbon_full_day_clean.csv'\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSample of final data:\")\n",
    "print(final_df_clean.head())\n",
    "\n",
    "final_df_clean.to_csv(\"carbon_full_day_clean.csv\", index=False)\n",
    "print(f\"\\nSaved to 'carbon_full_day_clean.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "316839bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compressed CSV\n",
    "final_df_clean.to_csv(\"carbon_full_day_clean.csv.gz\", compression='gzip', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
