{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ecd64f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47973505",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_range_chunked(start_date, end_date):\n",
    "    max_days = 30  # chunk size\n",
    "    start = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    while start <= end:\n",
    "        chunk_end = min(start + timedelta(days=max_days - 1), end)\n",
    "        s = start.strftime(\"%Y-%m-%dT00:00Z\")\n",
    "        e = chunk_end.strftime(\"%Y-%m-%dT23:30Z\")\n",
    "        \n",
    "        print(f\"Fetching data from {s} to {e}...\")\n",
    "        url = f\"https://api.carbonintensity.org.uk/intensity/{s}/{e}\"\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        records = []\n",
    "        for item in data['data']:\n",
    "            records.append({\n",
    "                'from': pd.to_datetime(item['from']),\n",
    "                'to': pd.to_datetime(item['to']),\n",
    "                'forecast': item['intensity']['forecast'],\n",
    "                'actual': item['intensity']['actual'],\n",
    "                'index': item['intensity']['index']\n",
    "            })\n",
    "        \n",
    "        df_chunk = pd.DataFrame(records)\n",
    "        all_data.append(df_chunk)\n",
    "        \n",
    "        start = chunk_end + timedelta(days=1)\n",
    "    \n",
    "    df_all = pd.concat(all_data).reset_index(drop=True)\n",
    "    df_all['day_of_week'] = df_all['to'].dt.day_name()\n",
    "    return df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c7b10df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIVOT FUNCTION (optional - keep if you need 48HH format)\n",
    "def pivot_to_48HH(df):\n",
    "    df['date'] = df['to'].dt.date\n",
    "    df = df.sort_values(['date', 'to']).reset_index(drop=True)\n",
    "    df['half_hour_slot'] = df.groupby('date').cumcount() + 1\n",
    "    df_pivot = df.pivot(index='date', columns='half_hour_slot', values='actual')\n",
    "    df_pivot.columns = [f'hh_{i}' for i in range(1, 49)]\n",
    "    df_pivot.reset_index(inplace=True)\n",
    "    return df_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11759eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching continuous carbon data for entire study period...\n",
      "Fetching data from 2023-01-01T00:00Z to 2023-01-30T23:30Z...\n",
      "Fetching data from 2023-01-31T00:00Z to 2023-03-01T23:30Z...\n",
      "Fetching data from 2023-03-02T00:00Z to 2023-03-31T23:30Z...\n",
      "Fetching data from 2023-04-01T00:00Z to 2023-04-30T23:30Z...\n",
      "Fetching data from 2023-05-01T00:00Z to 2023-05-30T23:30Z...\n",
      "Fetching data from 2023-05-31T00:00Z to 2023-06-29T23:30Z...\n",
      "Fetching data from 2023-06-30T00:00Z to 2023-07-29T23:30Z...\n",
      "Fetching data from 2023-07-30T00:00Z to 2023-08-28T23:30Z...\n",
      "Fetching data from 2023-08-29T00:00Z to 2023-09-27T23:30Z...\n",
      "Fetching data from 2023-09-28T00:00Z to 2023-10-27T23:30Z...\n",
      "Fetching data from 2023-10-28T00:00Z to 2023-11-26T23:30Z...\n",
      "Fetching data from 2023-11-27T00:00Z to 2023-12-26T23:30Z...\n",
      "Fetching data from 2023-12-27T00:00Z to 2024-01-25T23:30Z...\n",
      "Fetching data from 2024-01-26T00:00Z to 2024-02-24T23:30Z...\n",
      "Fetching data from 2024-02-25T00:00Z to 2024-03-25T23:30Z...\n",
      "Fetching data from 2024-03-26T00:00Z to 2024-04-24T23:30Z...\n",
      "Fetching data from 2024-04-25T00:00Z to 2024-05-24T23:30Z...\n",
      "Fetching data from 2024-05-25T00:00Z to 2024-06-23T23:30Z...\n",
      "Fetching data from 2024-06-24T00:00Z to 2024-07-23T23:30Z...\n",
      "Fetching data from 2024-07-24T00:00Z to 2024-08-22T23:30Z...\n",
      "Fetching data from 2024-08-23T00:00Z to 2024-09-21T23:30Z...\n",
      "Fetching data from 2024-09-22T00:00Z to 2024-10-21T23:30Z...\n",
      "Fetching data from 2024-10-22T00:00Z to 2024-11-20T23:30Z...\n",
      "Fetching data from 2024-11-21T00:00Z to 2024-12-20T23:30Z...\n",
      "Fetching data from 2024-12-21T00:00Z to 2024-12-31T23:30Z...\n",
      "Total carbon records fetched: 34966\n",
      "Date range: 2022-12-31 23:30:00+00:00 to 2024-12-31 23:30:00+00:00\n",
      "Missing actual values: 5\n",
      "Saved continuous carbon data to 'carbon_continuous.csv'\n"
     ]
    }
   ],
   "source": [
    "# FETCH CONTINUOUS DATA FOR ENTIRE STUDY PERIOD\n",
    "print(\"Fetching continuous carbon data for entire study period...\")\n",
    "carbon_continuous = fetch_range_chunked(\"2023-01-01\", \"2024-12-31\")\n",
    "\n",
    "print(f\"Total carbon records fetched: {len(carbon_continuous)}\")\n",
    "print(f\"Date range: {carbon_continuous['from'].min()} to {carbon_continuous['to'].max()}\")\n",
    "print(f\"Missing actual values: {carbon_continuous['actual'].isna().sum()}\")\n",
    "\n",
    "# Save the continuous dataset\n",
    "carbon_continuous.to_csv(\"carbon_continuous.csv\", index=False)\n",
    "print(\"Saved continuous carbon data to 'carbon_continuous.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "075db099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weekday records: 29966\n",
      "Sunday records: 5000\n",
      "\n",
      "Sample of continuous carbon data:\n",
      "                       from                        to  forecast  actual index  \\\n",
      "0 2022-12-31 23:30:00+00:00 2023-01-01 00:00:00+00:00        75    65.0   low   \n",
      "1 2023-01-01 00:00:00+00:00 2023-01-01 00:30:00+00:00        73    72.0   low   \n",
      "2 2023-01-01 00:30:00+00:00 2023-01-01 01:00:00+00:00        63    80.0   low   \n",
      "3 2023-01-01 01:00:00+00:00 2023-01-01 01:30:00+00:00        71    72.0   low   \n",
      "4 2023-01-01 01:30:00+00:00 2023-01-01 02:00:00+00:00        76    65.0   low   \n",
      "\n",
      "  day_of_week  \n",
      "0      Sunday  \n",
      "1      Sunday  \n",
      "2      Sunday  \n",
      "3      Sunday  \n",
      "4      Sunday  \n"
     ]
    }
   ],
   "source": [
    "# Optional: Create separate weekday/Sunday files if still needed for other analyses\n",
    "carbon_weekdays = carbon_continuous[carbon_continuous['day_of_week'] != 'Sunday'].copy()\n",
    "carbon_sundays = carbon_continuous[carbon_continuous['day_of_week'] == 'Sunday'].copy()\n",
    "\n",
    "carbon_weekdays.to_csv(\"carbon_weekdays.csv\", index=False)\n",
    "carbon_sundays.to_csv(\"carbon_sundays.csv\", index=False)\n",
    "\n",
    "print(f\"Weekday records: {len(carbon_weekdays)}\")\n",
    "print(f\"Sunday records: {len(carbon_sundays)}\")\n",
    "\n",
    "# Display sample data\n",
    "print(\"\\nSample of continuous carbon data:\")\n",
    "print(carbon_continuous.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e69980",
   "metadata": {},
   "source": [
    "# Merging the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dc2386e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0.1: Define date ranges\n",
    "pre_start = pd.Timestamp(\"2023-02-01 00:00:00\")\n",
    "pre_end   = pd.Timestamp(\"2024-01-31 23:59:59\")\n",
    "\n",
    "post_start = pd.Timestamp(\"2024-04-01 00:00:00\")\n",
    "post_end   = pd.Timestamp(\"2024-12-31 23:59:59\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dae96800",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_carbon_data_for_peak_hours(carbon_df):\n",
    "    \"\"\"Process carbon data to keep 30-minute intervals for 4-7 PM hours, filtered for electricity data periods only\"\"\"\n",
    "    # Make a copy to avoid SettingWithCopyWarning\n",
    "    carbon_df = carbon_df.copy()\n",
    "    \n",
    "    # Convert 'from' column to datetime and remove timezone info for consistency\n",
    "    carbon_df['from'] = pd.to_datetime(carbon_df['from']).dt.tz_localize(None)\n",
    "    \n",
    "    # Filter for periods where electricity data exists\n",
    "    # Pre period: 2023-02-02 to 2024-01-31\n",
    "    # Post period: 2024-04-01 to 2024-12-31\n",
    "    pre_mask = (carbon_df['from'] >= pd.Timestamp(\"2023-02-02 00:00:00\")) & \\\n",
    "               (carbon_df['from'] <= pd.Timestamp(\"2024-01-31 23:59:59\"))\n",
    "    \n",
    "    post_mask = (carbon_df['from'] >= pd.Timestamp(\"2024-04-01 00:00:00\")) & \\\n",
    "                (carbon_df['from'] <= pd.Timestamp(\"2024-12-31 23:59:59\"))\n",
    "    \n",
    "    # Keep only data from periods where electricity data exists\n",
    "    carbon_df = carbon_df[pre_mask | post_mask].copy()\n",
    "    \n",
    "    # Filter for 4-7 PM hours (16:00-18:30 to match your electricity filtering)\n",
    "    carbon_df = carbon_df[(carbon_df['from'].dt.hour >= 16) & (carbon_df['from'].dt.hour < 19)].copy()\n",
    "    \n",
    "    # Keep the 30-minute intervals - create a datetime_30min column for exact matching\n",
    "    carbon_df['datetime_30min'] = carbon_df['from']\n",
    "    \n",
    "    # Select only the columns we need\n",
    "    carbon_30min = carbon_df[['datetime_30min', 'actual']].copy()\n",
    "    carbon_30min.rename(columns={'actual': 'actual_intensity'}, inplace=True)\n",
    "    \n",
    "    return carbon_30min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "edb77b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_electricity_with_carbon(file_name, start_date, end_date, treatment, period, carbon_data):\n",
    "    \"\"\"\n",
    "    Process electricity data and merge with carbon data (keeping 30-minute intervals)\n",
    "    \"\"\"\n",
    "    # Load and preprocess electricity data\n",
    "    df = pd.read_csv(file_name)\n",
    "    if 'Unnamed: 0' in df.columns:\n",
    "        df = df.rename(columns={'Unnamed: 0': 'Time'})\n",
    "    else:\n",
    "        # Handle case where first column might have different name\n",
    "        df.rename(columns={df.columns[0]: 'Time'}, inplace=True)\n",
    "    \n",
    "    # Convert Time column to datetime\n",
    "    df['Time'] = pd.to_datetime(df['Time'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    print(f\"Processing {file_name} -> Min Time: {df['Time'].min()}, Max Time: {df['Time'].max()}\")\n",
    "    \n",
    "    # Filter by time range (pre or post)\n",
    "    df = df[(df['Time'] >= start_date) & (df['Time'] <= end_date)]\n",
    "    \n",
    "    # Keep only 4-7 PM consumption (matching your approach)\n",
    "    df = df[(df['Time'].dt.hour >= 16) & (df['Time'].dt.hour < 19)]\n",
    "    \n",
    "    # Convert from wide to long format\n",
    "    df_long = df.melt(id_vars=['Time'], var_name='ANON_ID', value_name='ELEC_KWH')\n",
    "    \n",
    "    # Remove duplicates after melting\n",
    "    df_long.drop_duplicates(subset=['Time', 'ANON_ID'], inplace=True)\n",
    "    \n",
    "    # Create datetime_30min column for exact matching with carbon data\n",
    "    df_long['datetime_30min'] = df_long['Time']\n",
    "    \n",
    "    # Merge with carbon data on exact 30-minute intervals\n",
    "    df_with_carbon = pd.merge(df_long, carbon_data, on='datetime_30min', how='left')\n",
    "    \n",
    "    # Calculate carbon emissions\n",
    "    df_with_carbon['carbon_emissions'] = df_with_carbon['ELEC_KWH'] * df_with_carbon['actual_intensity']\n",
    "    \n",
    "    # Add treatment and period columns\n",
    "    df_with_carbon['treatment'] = treatment\n",
    "    df_with_carbon['period'] = period\n",
    "    \n",
    "    # Create date column for easier analysis (but keep the 30-minute granularity)\n",
    "    df_with_carbon['date'] = df_with_carbon['Time'].dt.date.astype(str)\n",
    "    \n",
    "    # Select and reorder final columns\n",
    "    final_columns = ['ANON_ID', 'date', 'Time', 'ELEC_KWH', 'actual_intensity', 'carbon_emissions', 'treatment', 'period']\n",
    "    df_final = df_with_carbon[final_columns].copy()\n",
    "    \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ba8a3864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Carbon Data\n",
    "\n",
    "# Load the single continuous carbon file\n",
    "carbon_continuous = pd.read_csv('carbon_continuous.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "75773454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carbon data processed: 3822 30-minute interval records\n",
      "Carbon datetime range: 2023-02-02 16:00:00 to 2024-12-31 18:30:00\n",
      "Missing carbon intensity values: 0\n"
     ]
    }
   ],
   "source": [
    "# Process carbon data for peak hours (4-7 PM)\n",
    "all_carbon = process_carbon_data_for_peak_hours(carbon_continuous)\n",
    "\n",
    "print(f\"Carbon data processed: {len(all_carbon)} 30-minute interval records\")\n",
    "print(f\"Carbon datetime range: {all_carbon['datetime_30min'].min()} to {all_carbon['datetime_30min'].max()}\")\n",
    "print(f\"Missing carbon intensity values: {all_carbon['actual_intensity'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cd27075a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing D:\\LSE\\Capstone Project\\FLASH data\\controlePreConsolide.csv -> Min Time: 2023-02-02 00:00:00, Max Time: 2024-01-31 23:30:00\n",
      "Processing D:\\LSE\\Capstone Project\\FLASH data\\controleConsolide.csv -> Min Time: 2024-04-01 00:00:00, Max Time: 2024-12-31 23:30:00\n",
      "Processing D:\\LSE\\Capstone Project\\FLASH data\\interventionPreConsolide.csv -> Min Time: 2023-02-02 00:00:00, Max Time: 2024-01-31 23:30:00\n",
      "Processing D:\\LSE\\Capstone Project\\FLASH data\\interventionConsolide.csv -> Min Time: 2024-04-01 00:00:00, Max Time: 2024-12-31 23:30:00\n",
      "Individual dataset shapes:\n",
      "df_control_pre: (691200, 8)\n",
      "df_control_post: (713460, 8)\n",
      "df_intervention_pre: (839790, 8)\n",
      "df_intervention_post: (936396, 8)\n"
     ]
    }
   ],
   "source": [
    "# Process Electricity Data with Carbon Integration\n",
    "\n",
    "# Define your electricity files folder path\n",
    "electricity_folder = \"D:\\LSE\\Capstone Project\\FLASH data\" \n",
    "\n",
    "# Define file configurations: (filename, start_date, end_date, treatment, period)\n",
    "# Excluding Sunday files\n",
    "file_configs = [\n",
    "    (\"controlePreConsolide.csv\", pre_start, pre_end, \"Control\", \"Pre\"),\n",
    "    (\"controleConsolide.csv\", post_start, post_end, \"Control\", \"Post\"),\n",
    "    (\"interventionPreConsolide.csv\", pre_start, pre_end, \"Intervention\", \"Pre\"),\n",
    "    (\"interventionConsolide.csv\", post_start, post_end, \"Intervention\", \"Post\")\n",
    "]\n",
    "\n",
    "# Process all files in a loop (weekdays only)\n",
    "processed_datasets = {}\n",
    "dataset_names = [\"df_control_pre\", \"df_control_post\", \n",
    "                \"df_intervention_pre\", \"df_intervention_post\"]\n",
    "\n",
    "for i, (filename, start_date, end_date, treatment, period) in enumerate(file_configs):\n",
    "    file_path = os.path.join(electricity_folder, filename)\n",
    "    dataset_name = dataset_names[i]\n",
    "    \n",
    "    processed_datasets[dataset_name] = preprocess_electricity_with_carbon(\n",
    "        file_path, start_date, end_date, treatment, period, all_carbon)\n",
    "\n",
    "# Extract individual datasets for easier access (weekdays only)\n",
    "df_control_pre = processed_datasets[\"df_control_pre\"]\n",
    "df_control_post = processed_datasets[\"df_control_post\"]\n",
    "df_intervention_pre = processed_datasets[\"df_intervention_pre\"]\n",
    "df_intervention_post = processed_datasets[\"df_intervention_post\"]\n",
    "\n",
    "print(\"Individual dataset shapes:\")\n",
    "for name, df in processed_datasets.items():\n",
    "    print(f\"{name}: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "44b05187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final combined dataset shape: (3172362, 7)\n",
      "Date range: 2023-02-02 to 2024-12-31\n",
      "Unique households: 1173\n",
      "\n",
      "Sample of final data:\n",
      "                                             ANON_ID        date  ELEC_KWH  \\\n",
      "0  4dd3317694364b953434dc42eff7f9950095c4ab537c3b...  2023-02-02     0.243   \n",
      "1  4dd3317694364b953434dc42eff7f9950095c4ab537c3b...  2023-02-02     0.292   \n",
      "2  4dd3317694364b953434dc42eff7f9950095c4ab537c3b...  2023-02-02     0.260   \n",
      "3  4dd3317694364b953434dc42eff7f9950095c4ab537c3b...  2023-02-02     0.541   \n",
      "4  4dd3317694364b953434dc42eff7f9950095c4ab537c3b...  2023-02-02     0.400   \n",
      "\n",
      "   actual_intensity  carbon_emissions treatment period  \n",
      "0             128.0            31.104   Control    Pre  \n",
      "1             132.0            38.544   Control    Pre  \n",
      "2             125.0            32.500   Control    Pre  \n",
      "3             121.0            65.461   Control    Pre  \n",
      "4             125.0            50.000   Control    Pre  \n"
     ]
    }
   ],
   "source": [
    "# Combine all datasets (weekdays only)\n",
    "all_datasets = [\n",
    "    df_control_pre, df_control_post,\n",
    "    df_intervention_pre, df_intervention_post\n",
    "]\n",
    "\n",
    "# Filter out empty datasets\n",
    "all_datasets = [df for df in all_datasets if not df.empty]\n",
    "\n",
    "# Combine all data\n",
    "final_df = pd.concat(all_datasets, ignore_index=True)\n",
    "\n",
    "# Reorder columns to match your desired format\n",
    "final_df = final_df[['ANON_ID', 'date', 'ELEC_KWH', 'actual_intensity', 'carbon_emissions', 'treatment', 'period']]\n",
    "\n",
    "# Remove rows where carbon data is missing\n",
    "final_df = final_df.dropna(subset=['actual_intensity'])\n",
    "\n",
    "print(f\"\\nFinal combined dataset shape: {final_df.shape}\")\n",
    "print(f\"Date range: {final_df['date'].min()} to {final_df['date'].max()}\")\n",
    "print(f\"Unique households: {final_df['ANON_ID'].nunique()}\")\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nSample of final data:\")\n",
    "print(final_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a5cbc12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data saved to 'carbon_electricity_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the final dataset\n",
    "final_df.to_csv('carbon_electricity_data.csv', index=False)\n",
    "print(\"\\nData saved to 'carbon_electricity_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b8afc566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing electricity values: 146841\n",
      "Missing carbon emissions: 146841\n",
      "Percentage of missing data: 4.63%\n"
     ]
    }
   ],
   "source": [
    "# Check missing values in final dataset\n",
    "print(f\"Missing electricity values: {final_df['ELEC_KWH'].isna().sum()}\")\n",
    "print(f\"Missing carbon emissions: {final_df['carbon_emissions'].isna().sum()}\")\n",
    "print(f\"Percentage of missing data: {final_df['ELEC_KWH'].isna().mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4f8ec817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset: 3,172,362 records\n",
      "Clean dataset: 3,025,521 records\n",
      "Records removed: 146,841\n"
     ]
    }
   ],
   "source": [
    "# Remove records with missing electricity data\n",
    "final_df_clean = final_df.dropna(subset=['ELEC_KWH'])\n",
    "\n",
    "print(f\"Original dataset: {len(final_df):,} records\")\n",
    "print(f\"Clean dataset: {len(final_df_clean):,} records\")\n",
    "print(f\"Records removed: {len(final_df) - len(final_df_clean):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1f1f2797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean dataset saved to 'carbon_electricity_data_clean.csv'\n"
     ]
    }
   ],
   "source": [
    "# Save the cleaned dataset\n",
    "final_df_clean.to_csv('carbon_electricity_data_clean.csv', index=False)\n",
    "print(\"Clean dataset saved to 'carbon_electricity_data_clean.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
